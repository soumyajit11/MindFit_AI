# -*- coding: utf-8 -*-
"""mentalHealthChatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HhfJr88vOHCX5znScUqL-wt0HozcKoLL
"""

!pip install langchain_groq langchain_core langchain_community

from langchain_groq import ChatGroq
llm= ChatGroq(temperature=0, groq_api_key="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", model_name="llama3-70b-8192")
result=llm.invoke("History of Kolkata")
print(result.content)

!pip install pypdf

!pip install chromadb

!pip install sentence_transformers

from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os

def initialize_llm():
  llm=ChatGroq(temperature=0, groq_api_key="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", model_name="llama-3.3-70b-versatile")
  return llm

def create_vector_db():
  loader=DirectoryLoader("/content/data/", glob='*.pdf', loader_cls=PyPDFLoader)
  documents=loader.load()
  text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
  texts=text_splitter.split_documents(documents)
  embeddings=HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
  vector_db=Chroma.from_documents(texts, embeddings, persist_directory='./chroma_db')
  vector_db.persist()

  print("ChromaDB created and saved")

  return vector_db

def setup_qa_chain(vector_db,llm):
  retriever=vector_db.as_retriever()
  prompt_templates=""""You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
  {context}
  User={question}
  Chatbot:"""
  prompts= PromptTemplate(template=prompt_templates,input_variables=['context','question'])
  qa_chain=RetrievalQA.from_chain_type(llm=llm, chain_type='stuff', retriever=retriever, chain_type_kwargs={'prompt':prompts})

  return qa_chain

def main():
    print("Initialising Chatbot....")
    llm=initialize_llm()

    db_path="/content/chroma_db"

    if not os.path.exists(db_path):
      vector_db=create_vector_db()
    else:
      embeddings=HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
      vector_db=Chroma(persist_directory=db_path,embedding_function=embeddings)

    qa_chain=setup_qa_chain(vector_db,llm)

    while True:
      query=input("\nHuman: ")
      if query.lower()=="exit":
        print("CHATBOT take care of yourself. Goodbye!")
        break
      response= qa_chain.run(query)
      print(f"Chatbot: {response}")

if __name__=="__main__":
  main()

!pip install gradio

import os
import gradio as gr
from langchain_groq import ChatGroq
from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

# === Initialize LLM ===
def initialize_llm():
    return ChatGroq(
        temperature=0,
        groq_api_key=os.getenv("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"),
        model_name="llama-3-70b-8192"
    )

# === Create Vector DB ===
def create_vector_db():
    loader = DirectoryLoader("data", glob="*.pdf", loader_cls=PyPDFLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    db = Chroma.from_documents(texts, embeddings, persist_directory="chroma_db")
    db.persist()
    return db

# === Setup QA chain ===
def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt_template = """You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
{context}
User: {question}
Chatbot:"""
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    return RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, chain_type_kwargs={"prompt": prompt})

# === Initialize ===
llm = initialize_llm()

if not os.path.exists("chroma_db"):
    vector_db = create_vector_db()
else:
    embeddings = HuggingFaceBgeEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma(persist_directory="chroma_db", embedding_function=embeddings)

qa_chain = setup_qa_chain(vector_db, llm)

# === Gradio UI ===
def chatbot_response(user_input, history=[]):
    if not user_input.strip():
        return "Please provide a valid input", history
    response = qa_chain.run(user_input)
    history.append((user_input, response))
    return "", history

with gr.Blocks() as app:
    gr.ChatInterface(fn=chatbot_response, title="Mental Health Chatbot")

app.launch(share=True)